{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIwZsU0haNkj"
      },
      "source": [
        "## Task 1 - SQL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45wCuSvgajao"
      },
      "source": [
        "### Build SparkSession:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JwcbnqiymCE3"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/06/14 23:03:56 WARN Utils: Your hostname, sam-pc resolves to a loopback address: 127.0.1.1; using 192.168.1.2 instead (on interface wlp2s0)\n",
            "22/06/14 23:03:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
            "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "22/06/14 23:04:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.appName(\"task1\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8HITwTqnJcX"
      },
      "source": [
        "### Read the json file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "93iqAB7tnMYQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "js_file = spark.read.json('DataFrames_sample.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNx0qMfunbKX"
      },
      "source": [
        "### Display the schema:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UG4CcVJenc9y"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- D: double (nullable = true)\n",
            " |-- H: double (nullable = true)\n",
            " |-- HDD: string (nullable = true)\n",
            " |-- Id: long (nullable = true)\n",
            " |-- Model: string (nullable = true)\n",
            " |-- RAM: string (nullable = true)\n",
            " |-- ScreenSize: string (nullable = true)\n",
            " |-- W: double (nullable = true)\n",
            " |-- Weight: double (nullable = true)\n",
            " |-- Year: long (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "js_file.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zaj0nHTcngEF"
      },
      "source": [
        "### Get all the data when \"Model\" equal \"MacBook Pro\":\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Vm9QPKBCnkuS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "js_file.select('*').where('Model == \"MacBook Pro\"').show()\n",
        "# js_file.where('Model == \"MacBook Pro\"').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43oLte9LuGzA"
      },
      "source": [
        "### Create Table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nVFYFcjtdIGW"
      },
      "outputs": [],
      "source": [
        "js_file.createOrReplaceTempView(\"products\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCLMmjRLdjbT"
      },
      "source": [
        "### Display \"RAM\"column and count \"RAM\" column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "BxykutRjuF0X"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----------+\n",
            "| RAM|count(RAM)|\n",
            "+----+----------+\n",
            "|64GB|         1|\n",
            "|16GB|         1|\n",
            "| 8GB|         2|\n",
            "+----+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select RAM, count(RAM) from Products group by RAM\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5nlwq4t9gvK"
      },
      "source": [
        "### Get all columns when \"Year\" column equal \"2015\"  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "WXxjFxN19hJl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql(\"select * from Products where Year==2015\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHjK2Kqfuv24"
      },
      "source": [
        "### Get all when \"Model\" start with \"M\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "m30EkY_iu1Gs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
            "|7.74|0.52|256GB SSD|  2|    MacBook| 8GB|       12\"|11.04|  2.03|2016|\n",
            "|8.94|0.68|128GB SSD|  3|MacBook Air| 8GB|     13.3\"| 12.8|  2.96|2016|\n",
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('Select * from Products where Model like \"M%\"').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igw9iqJQ7TdH"
      },
      "source": [
        "### Get all data when \"Model\" column equal \"MacBook Pro\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "SRCGSB_W9cPc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
            "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# spark.sql('Select * from Products where Model like \"MacBook Pro\"').show()\n",
        "spark.sql('Select * from Products where Model == \"MacBook Pro\"').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZIlmJidw1Ep"
      },
      "source": [
        "### Get all data with Multiple Conditions when \"RAM\" column equal \"8GB\" and \"Model\" column is \"Macbook\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "-5T7roxgnBBV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
            "|   D|   H|      HDD| Id|  Model|RAM|ScreenSize|    W|Weight|Year|\n",
            "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
            "|7.74|0.52|256GB SSD|  2|MacBook|8GB|       12\"|11.04|  2.03|2016|\n",
            "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# js_file.where('Model==\"MacBook\" and RAM==\"8GB\"').show()\n",
        "spark.sql('Select * from Products where Model==\"MacBook\" and RAM==\"8GB\"').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qk8YPAWQ8HxI"
      },
      "source": [
        "### Get all data with Multiple Conditions when \"D\" greater than or equal \"8\" and \"Model\" column is \"iMac\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "XDHJSpQK9MuS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---+----+-------+---+-----+----+----------+----+------+----+\n",
            "|  D|   H|    HDD| Id|Model| RAM|ScreenSize|   W|Weight|Year|\n",
            "+---+----+-------+---+-----+----+----------+----+------+----+\n",
            "|8.0|20.3|1TB SSD|  4| iMac|64GB|       27\"|25.6|  20.8|2017|\n",
            "+---+----+-------+---+-----+----+----------+----+------+----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('Select * from Products where Model==\"iMac\" and D>=8').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d6364f6"
      },
      "source": [
        "## Task 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlWhDvTPfZgu"
      },
      "source": [
        "### Read \"test1\" dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {
        "id": "7964d064"
      },
      "outputs": [],
      "source": [
        "test1_file = spark.read.csv('test1.csv', header=True,  inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJKjDOKHfnCt"
      },
      "source": [
        "### Display Salary of the people less than or equal to 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "c21edffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test1_file.select('*').where('Salary<=20000').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvFWNFJjf0Pq"
      },
      "source": [
        "### Display Salary of the people less than or equal to 20000 and greater than or equal 15000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "26f76ee1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+----------+------+\n",
            "|   Name|age|Experience|Salary|\n",
            "+-------+---+----------+------+\n",
            "|  Sunny| 29|         4| 20000|\n",
            "|   Paul| 24|         3| 20000|\n",
            "| Harsha| 21|         1| 15000|\n",
            "|Shubham| 23|         2| 18000|\n",
            "+-------+---+----------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test1_file.select('*').where('Salary between 15000 and 20000').show()\n",
        "# test1_file.select('*').where('Salary >= 15000 and Salary<= 20000').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VAcIXkTgN9D"
      },
      "source": [
        "## Task 3 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOeqRO2KgW34"
      },
      "source": [
        "### Read \"test3\" dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {
        "id": "4d3bd081"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        }
      ],
      "source": [
        "test3_file = spark.read.csv('test3.csv', header=True, inferSchema=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejyUT1rngdeR"
      },
      "source": [
        "### Display dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "metadata": {
        "id": "7ed791ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------------+------+\n",
            "|     Name| Departments|salary|\n",
            "+---------+------------+------+\n",
            "|    Krish|Data Science| 10000|\n",
            "|    Krish|         IOT|  5000|\n",
            "|   Mahesh|    Big Data|  4000|\n",
            "|    Krish|    Big Data|  4000|\n",
            "|   Mahesh|Data Science|  3000|\n",
            "|Sudhanshu|Data Science| 20000|\n",
            "|Sudhanshu|         IOT| 10000|\n",
            "|Sudhanshu|    Big Data|  5000|\n",
            "|    Sunny|Data Science| 10000|\n",
            "|    Sunny|    Big Data|  2000|\n",
            "+---------+------------+------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test3_file.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp42YtorghXJ"
      },
      "source": [
        "### Display schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "id": "d57d24ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Departments: string (nullable = true)\n",
            " |-- salary: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test3_file.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import sum as sumFun, avg as avgFun, count as countFun, col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test3_file.createTempView('Employee')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHxWeGCCgnww"
      },
      "source": [
        "### Group by \"Name\" column and using sum function on \"Name\" column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+\n",
            "|     Name|Avg_Salary|\n",
            "+---------+----------+\n",
            "|Sudhanshu|     35000|\n",
            "|    Sunny|     12000|\n",
            "|    Krish|     19000|\n",
            "|   Mahesh|      7000|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test3_file.groupBy('Name').agg(sumFun('Salary').alias('Avg_Salary')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------------+\n",
            "|     Name|Total_Salary|\n",
            "+---------+------------+\n",
            "|Sudhanshu|     35000.0|\n",
            "|    Sunny|     12000.0|\n",
            "|    Krish|     19000.0|\n",
            "|   Mahesh|      7000.0|\n",
            "+---------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('''select Name, sum(Salary) as Total_Salary\n",
        "                from Employee \n",
        "                group by Name''').show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWgkaU3bhUOL"
      },
      "source": [
        "### Group by \"Name\" column and using avg function on \"Name\" column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+------------------+\n",
            "|     Name|       avg(Salary)|\n",
            "+---------+------------------+\n",
            "|Sudhanshu|11666.666666666666|\n",
            "|    Sunny|            6000.0|\n",
            "|    Krish| 6333.333333333333|\n",
            "|   Mahesh|            3500.0|\n",
            "+---------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test3_file.groupBy('Name').avg('Salary').alias('Avg_Salary').show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+\n",
            "|     Name|Avg_Salary|\n",
            "+---------+----------+\n",
            "|Sudhanshu|  11666.67|\n",
            "|    Sunny|    6000.0|\n",
            "|    Krish|   6333.33|\n",
            "|   Mahesh|    3500.0|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test3_file.groupBy('Name').agg(round(avgFun('Salary'), 2).alias('Avg_Salary')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "fc122ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+----------+\n",
            "|     Name|Avg_Salary|\n",
            "+---------+----------+\n",
            "|Sudhanshu|  11666.67|\n",
            "|    Sunny|    6000.0|\n",
            "|    Krish|   6333.33|\n",
            "|   Mahesh|    3500.0|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('''select Name, round(avg(Salary), 2) as Avg_Salary \n",
        "                from Employee \n",
        "                group by Name''').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARg_-WPKhfL5"
      },
      "source": [
        "### Group by \"Departments\" column and using sum function on \"Departments\" column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+\n",
            "| Departments|Total_Salary|\n",
            "+------------+------------+\n",
            "|         IOT|       15000|\n",
            "|    Big Data|       15000|\n",
            "|Data Science|       43000|\n",
            "+------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test3_file.groupBy('Departments').agg(sumFun('Salary').alias('Total_Salary')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "151d2264"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------+\n",
            "| Departments|Total_Salary|\n",
            "+------------+------------+\n",
            "|         IOT|     15000.0|\n",
            "|    Big Data|     15000.0|\n",
            "|Data Science|     43000.0|\n",
            "+------------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('''select Departments, sum(Salary) as Total_Salary \n",
        "                from Employee \n",
        "                group by Departments''').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7rdLSEXhn4W"
      },
      "source": [
        "### Group by \"Departments\" column and using mean function on \"Departments\" column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+----------+\n",
            "| Departments|Avg_Salary|\n",
            "+------------+----------+\n",
            "|         IOT|    7500.0|\n",
            "|    Big Data|    3750.0|\n",
            "|Data Science|   10750.0|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test3_file.groupBy('Departments').agg(avgFun('Salary').alias('Avg_Salary')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "66fe5552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+----------+\n",
            "| Departments|Avg_Salary|\n",
            "+------------+----------+\n",
            "|         IOT|    7500.0|\n",
            "|    Big Data|    3750.0|\n",
            "|Data Science|   10750.0|\n",
            "+------------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('''select Departments, Avg(Salary) as Avg_Salary \n",
        "                from Employee \n",
        "                group by Departments''').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bndivgGjhsbq"
      },
      "source": [
        "### Group by \"Departments\" column and using count function on \"Departments\" column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----+\n",
            "| Departments|Count|\n",
            "+------------+-----+\n",
            "|         IOT|    2|\n",
            "|    Big Data|    4|\n",
            "|Data Science|    4|\n",
            "+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test3_file.groupBy('Departments').agg(countFun('Salary').alias('Count')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "bc7bf192"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+-----+\n",
            "| Departments|Count|\n",
            "+------------+-----+\n",
            "|         IOT|    2|\n",
            "|    Big Data|    4|\n",
            "|Data Science|    4|\n",
            "+------------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('''select Departments, count(*) as Count \n",
        "                from Employee \n",
        "                group by Departments''').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfPs99wnhwGu"
      },
      "source": [
        "### Apply agg to using sum function get the total of salaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|Total_Salary|\n",
            "+------------+\n",
            "|       73000|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "test3_file.agg(sumFun('Salary').alias('Total_Salary')).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "id": "37b26cbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+\n",
            "|Total_Salary|\n",
            "+------------+\n",
            "|     73000.0|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "spark.sql('''select sum(Salary) as Total_Salary \n",
        "                from Employee ''').show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYD0wGPRi1FO"
      },
      "source": [
        "## Task 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJLc1PY1i-Np"
      },
      "source": [
        "You've been flown to their headquarters in Ulsan, South Korea, to assist them in accurately estimating the number of crew members a ship will need.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PEoknoejL4r"
      },
      "source": [
        "They're currently building new ships for certain customers, and they'd like you to create a model and utilize it to estimate how many crew members the ships will require.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70slYH-tjR81"
      },
      "source": [
        "Metadata:\n",
        "1. Measurements of ship size \n",
        "2. capacity \n",
        "3. crew \n",
        "4. age for 158 cruise ships."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzrhGnHkRCU"
      },
      "source": [
        "It is saved in a csv file for you called \"ITI_data.csv\". our task is to develop a regression model that will assist in predicting the number of crew members required for future ships. The client also indicated that they have found that particular cruise lines will differ in acceptable crew counts, thus this is most likely an important factor to consider when conducting your investigation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 401,
      "metadata": {
        "id": "A9CZzWWqZnOC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
            "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|\n",
            "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
            "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
            "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
            "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|\n",
            "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|\n",
            "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|\n",
            "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ship_df = spark.read.csv('ITI_data.csv', inferSchema=True, header=True)\n",
        "ship_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 402,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- Ship_name: string (nullable = true)\n",
            " |-- Cruise_line: string (nullable = true)\n",
            " |-- Age: integer (nullable = true)\n",
            " |-- Tonnage: double (nullable = true)\n",
            " |-- passengers: double (nullable = true)\n",
            " |-- length: double (nullable = true)\n",
            " |-- cabins: double (nullable = true)\n",
            " |-- passenger_density: double (nullable = true)\n",
            " |-- crew: double (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ship_df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### splitt test and train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 420,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((130, 9), (28, 9))"
            ]
          },
          "execution_count": 420,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainDF, testDF = ship_df.randomSplit([0.8, 0.2], seed=2021)\n",
        "trainDF.toPandas().shape, testDF.toPandas().shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QTNLhZSlf9_"
      },
      "source": [
        "### OneHotEncoder \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 403,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 421,
      "metadata": {},
      "outputs": [],
      "source": [
        "catCols = ship_df.toPandas().select_dtypes(include='object').columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 422,
      "metadata": {
        "id": "-ZZxxxKLZnOF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Ship_name_Index', 'Cruise_line_Index']"
            ]
          },
          "execution_count": 422,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "indexOutputCols = [x + \"_Index\" for x in catCols]\n",
        "indexOutputCols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 423,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Ship_name_OHE', 'Cruise_line_OHE']"
            ]
          },
          "execution_count": 423,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oheOutputCols = [x + \"_OHE\" for x in catCols]\n",
        "oheOutputCols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 425,
      "metadata": {},
      "outputs": [],
      "source": [
        "stringIndexer = StringIndexer(inputCols=catCols,\n",
        "                              outputCols=indexOutputCols,\n",
        "                             handleInvalid='skip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 426,
      "metadata": {},
      "outputs": [],
      "source": [
        "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
        "                          outputCols=oheOutputCols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 429,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['cabins',\n",
              " 'crew',\n",
              " 'Tonnage',\n",
              " 'passengers',\n",
              " 'passenger_density',\n",
              " 'Age',\n",
              " 'length']"
            ]
          },
          "execution_count": 429,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "numCols = list(set(ship_df.columns) - set(catCols))\n",
        "numCols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 430,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Ship_name_OHE',\n",
              " 'Cruise_line_OHE',\n",
              " 'cabins',\n",
              " 'crew',\n",
              " 'Tonnage',\n",
              " 'passengers',\n",
              " 'passenger_density',\n",
              " 'Age',\n",
              " 'length']"
            ]
          },
          "execution_count": 430,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assemblerInputs = oheOutputCols + numCols\n",
        "assemblerInputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNCxWem0l662"
      },
      "source": [
        "### Use VectorAssembler to merge all columns into one column:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 431,
      "metadata": {
        "collapsed": true,
        "id": "pE4ohNjVZnOG"
      },
      "outputs": [],
      "source": [
        "vecAssembler = VectorAssembler(inputCols = assemblerInputs,\n",
        "                                outputCol = 'features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbf56f6AmUUl"
      },
      "source": [
        "### Create a Linear Regression Model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainDFF = trainDF\n",
        "testDFF = testDF\n",
        "\n",
        "for s in catCols:\n",
        "    trainDFF = trainDF.drop(s)\n",
        "    testDFF = testDF.drop(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {},
      "outputs": [],
      "source": [
        "assumVec = VectorAssembler(inputCols = numCols, outputCol = 'features')\n",
        "output_train=assumVec.transform(trainDFF)\n",
        "output_test=assumVec.transform(testDFF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {
        "id": "nvqnqTkunkNx"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {},
      "outputs": [],
      "source": [
        "linearR = LinearRegression(featuresCol=\"features\", \n",
        "                            labelCol=\"crew\", \n",
        "                            predictionCol=\"prediction\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/06/15 03:00:00 WARN Instrumentation: [8e5a6b87] regParam is zero, which might cause numerical instability and overfitting.\n"
          ]
        }
      ],
      "source": [
        "outLR = linearR.fit(output_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Squrer : 1.0 | adj R2 Squrer : 1.0 Abs Error : 9.300252875513812e-16\n"
          ]
        }
      ],
      "source": [
        "#evaluating train data\n",
        "ship_results = outLR.evaluate(output_train)\n",
        "print('R2 Squrer :',ship_results.r2, '| adj R2 Squrer :',ship_results.r2adj, 'Abs Error :',ship_results.meanAbsoluteError)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 467,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R2 Squrer : 1.0 | adj R2 Squrer : 1.0 Abs Error : 8.009466106224344e-16\n"
          ]
        }
      ],
      "source": [
        "#evaluating train data\n",
        "ship_results = outLR.evaluate(output_test)\n",
        "print('R2 Squrer :',ship_results.r2, '| adj R2 Squrer :',ship_results.r2adj, 'Abs Error :',ship_results.meanAbsoluteError)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVdxQTcSC6Cz"
      },
      "source": [
        "### Creating a Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 434,
      "metadata": {
        "id": "UqM9HxkNIHwE"
      },
      "outputs": [],
      "source": [
        "pipline = Pipeline(stages=[stringIndexer, oheEncoder, vecAssembler, linearR])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "22/06/15 02:51:41 WARN Instrumentation: [02b9c999] regParam is zero, which might cause numerical instability and overfitting.\n",
            "22/06/15 02:51:41 WARN Instrumentation: [02b9c999] Cholesky solver failed due to singular covariance matrix. Retrying with Quasi-Newton solver.\n"
          ]
        }
      ],
      "source": [
        "pipelineModel = pipline.fit(trainDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 436,
      "metadata": {},
      "outputs": [],
      "source": [
        "predDF = pipelineModel.transform(testDF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 438,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------------------+-----+------------------+\n",
            "|            features| crew|        prediction|\n",
            "+--------------------+-----+------------------+\n",
            "|(141,[56,116,134,...|  9.3| 9.139475516786993|\n",
            "|(141,[56,117,134,...|  7.2| 7.333511915401142|\n",
            "|(141,[10,116,134,...|10.29|10.015154196575764|\n",
            "|(141,[10,134,135,...|  1.6|1.5291777924528374|\n",
            "+--------------------+-----+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "predDF.select('features','crew','prediction').show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SluXM6j-Gt9B"
      },
      "source": [
        "By Eng. Mostafa Nabieh \n",
        "If you have questions, please feel free to ask.\n",
        "\n",
        "My Email : nabieh.mostafa@yahoo.com\n",
        "\n",
        "My Whatsapp : +201015197566"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Session 2 H.W.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
